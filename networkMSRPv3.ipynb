{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"networkMSRPv3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"MdEi2yFVjcEs","colab":{}},"source":["import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ENru5ixojhTY","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zsh5PhS8jyC4","colab":{}},"source":["cd drive/My Drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GOL_xMwHj4NQ","colab":{}},"source":["cd MSRP"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lDJU55K0TYv","colab_type":"text"},"source":["# **IMPORTING LIBRARIES**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uSRaRsjPjhbq","colab":{}},"source":["from data import Data\n","from sklearn.metrics import f1_score\n","import sklearn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HYTshg6ZjcEv","colab":{}},"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fXhQ1uUejcEx","colab":{}},"source":["use_cuda = torch.cuda.is_available()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JN1k4zlpjcE0","colab":{}},"source":["data_file = \"./dataset/train.tsv\"\n","data_test_file = \"./dataset/test.tsv\"\n","training_ratio = 0.9999999\n","max_len = 20\n","tracking_pair = False\n","hidden_size = 50\n","batch_size = 16\n","num_iters = 10\n","learning_rate = 0.001"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wewXsuSa0nAH","colab_type":"text"},"source":["# DATA"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5dokmAXejcE2","colab":{}},"source":["data = Data(data_file,data_test_file,training_ratio,max_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lHFnXIKfjcE6","colab":{}},"source":["len(data.word2index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_Y9erOS2jcE9","colab":{}},"source":["print('Number of training samples        :', len(data.x_train))\n","print('Number of validation samples      :', len(data.x_val))\n","print('Maximum sequence length           :', max_len)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2GbUg_x10rkZ","colab_type":"text"},"source":["# Embeddings"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z_rn2-1xjcE_","colab":{}},"source":["embd_file = \"./glove-global-vectors-for-word-representation/glove.6B.100d.txt\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gbDQBbW8jcFC","colab":{}},"source":["from embedding_helper2 import Get_Embedding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fnO4rIh6jcFE","colab":{}},"source":["embedding = Get_Embedding(embd_file, data.word2index)\n","embedding_size = embedding.embedding_matrix.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8CdmMYJCjcFG","colab":{}},"source":["embedding_size"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-Nm8FtB36PN","colab_type":"code","colab":{}},"source":["len(embedding.embedding_matrix[7])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wDrorlFIjcFJ","colab":{}},"source":["len(embedding.embedding_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VPpaUlp-jcFM","colab":{}},"source":["import torch.nn as nn\n","from torch import Tensor\n","from torch import optim\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3bi1BNwcjcFR","colab":{}},"source":["def commonWords(sen_1, sen_2):\n","  d = np.empty(len(data.word2index), dtype=int)\n","  for i in range(len(d)):\n","    d[i] = -1\n","    \n","  listPairs = []\n","  list1 = []\n","  list2 = []\n","  for i in range(len(sen_1)):\n","    d[sen_1[i]] = i\n","    \n","  for i in range(len(sen_2)):\n","    if d[sen_2[i]] > 1 and sen_2[i] > 0 :\n","      list1.append(d[sen_2[i]])\n","      list2.append(i)\n","    \n","  list1 = list(dict.fromkeys(list1))\n","  list2 = list(dict.fromkeys(list2))\n","  \n","  listPairs.append(list1)\n","  listPairs.append(list2)\n","  return listPairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"klC-DZwpgja0","colab":{}},"source":["def max_pool(e_list):\n","  e_list = np.array(e_list)\n","  \n","  for i in range(len(e_list)):\n","    e_list[i] = e_list[i].data.cpu().numpy()\n","  mp = []\n","  for i in range(100):\n","    m = e_list[0][i]\n","    for j in range(len(e_list)):\n","      m = max(m, e_list[j][i])\n","    mp.append(m)\n","      \n","  #print(\"Length of mp = \" + str(len(mp)))\n","  return torch.cuda.FloatTensor(mp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DTPJek_J0xZC","colab_type":"text"},"source":["# GAN MODEL"]},{"cell_type":"code","metadata":{"id":"UvJueIzjrGBz","colab_type":"code","colab":{}},"source":["def weights_init(m):\n","  classname = m.__class__.__name__\n","  if classname.find('Linear') != -1:\n","    nn.init.xavier_normal_(m.weight.data, 1.0, 0.02).cuda()\n","    nn.init.constant_(m.bias.data, 0).cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AU43lclXQCeZ","colab_type":"code","colab":{}},"source":["learning_rate_G = 0.001\n","learning_rate_D = 0.001\n","learning_rate_F = 0.001"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAPzKwA4jvg8","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.use_cuda = torch.cuda.is_available()\n","        self.main = nn.Sequential(\n","            nn.Linear(100, 100),\n","            nn.Tanh()\n","        )\n","    def forward(self, input):\n","        return self.main(input).cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DTE2QxQmn5AC","colab_type":"code","colab":{}},"source":["netG = Generator()\n","if use_cuda: netG = netG.cuda()\n","netG.apply(weights_init)\n","print(netG)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kks4pd_dj6oJ","colab_type":"code","colab":{}},"source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","      super(Discriminator, self).__init__()\n","      self.use_cuda = torch.cuda.is_available()\n","      self.main = nn.Sequential(\n","        nn.Linear(100, 2),\n","        nn.Softmax(dim = 1)\n","      ) \n","    def forward(self, input):\n","      return self.main(input).cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"elNwrRMXn3l7","colab_type":"code","colab":{}},"source":["netD = Discriminator()\n","if use_cuda: netD = netD.cuda()\n","netD.apply(weights_init)\n","print(netD)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kopV8vt32TnZ","colab_type":"code","colab":{}},"source":["real_label = torch.tensor([0,1])\n","fake_label = torch.tensor([1,0])\n","optimizerD = optim.Adam(netD.parameters(), lr=learning_rate_G)\n","optimizerG = optim.Adam(netG.parameters(), lr=learning_rate_D)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjOLSAxY-yzI","colab_type":"code","colab":{}},"source":["class Dropout_layer(nn.Module):\n","  def __init__(self):\n","    super(Dropout_layer, self).__init__()\n","    self.d = nn.Dropout(p=0.1)\n","    \n","  def forward(self, input):\n","    return self.d(input).cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ugdntQY_9Rq","colab_type":"code","colab":{}},"source":["dropout_layer = Dropout_layer()\n","if use_cuda: dropout_layer = dropout_layer.cuda()\n","print(dropout_layer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5aDONvwvHnX","colab_type":"code","colab":{}},"source":["class Final_layer(nn.Module):\n","    def __init__(self):\n","        super(Final_layer, self).__init__()\n","        self.use_cuda = torch.cuda.is_available()\n","        self.main = nn.Sequential(\n","            nn.Linear(101, 2),\n","            nn.Softmax(dim = 1)\n","        )\n","    def forward(self, input):\n","        return self.main(input).cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"85twnk_vvW_D","colab_type":"code","colab":{}},"source":["net_final = Final_layer()\n","if use_cuda: net_final = net_final.cuda()\n","net_final.apply(weights_init)\n","print(net_final)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lF8rw63dwAlu","colab_type":"code","colab":{}},"source":["optimizer_final = optim.Adam(net_final.parameters(), lr = learning_rate_F)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ju83n78m00Cp","colab_type":"text"},"source":["# MALSTM MODEL"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RyWhvPUrjcFT","colab":{}},"source":["class Manhattan_LSTM(nn.Module):\n","    def __init__(self, hidden_size, embedding, train_embedding = False):\n","        super(Manhattan_LSTM, self).__init__()\n","        self.use_cuda = torch.cuda.is_available()\n","        self.hidden_size = hidden_size\n","        \n","        self.embedding = nn.Embedding(embedding.shape[0], embedding.shape[1])\n","        self.embedding.weight = nn.Parameter(embedding)\n","        self.input_size = embedding.shape[1]\n","        \n","        self.embedding.weight.requires_grad = train_embedding\n","        \n","        self.lstm_1 = nn.LSTM(self.input_size, self.hidden_size, num_layers=1, bidirectional=True)\n","        self.lstm_2 = nn.LSTM(self.input_size, self.hidden_size, num_layers=1, bidirectional=True)\n","        \n","    def exponent_neg_manhattan_distance(self, x1, x2):\n","        return torch.exp(-torch.sum(torch.abs(x1 - x2), dim=1))\n","    \n","    def forward(self, input, hidden):\n","        \n","        #print(input[0])\n","        #print(input[1])\n","        \n","        ip0 = input[0].t()\n","        ip1 = input[1].t()\n","        \n","        commonList = []\n","        \n","        for i in range(batch_size):\n","            listPairs = commonWords(ip0[i], ip1[i])\n","            commonList.append(listPairs)\n","    \n","        commonList = np.array(commonList)\n","        \n","        #print(commonList)\n","        input_len = len(input[1])\n","        \n","        embedded_1 = self.embedding(input[0])\n","        embedded_2 = self.embedding(input[1])\n","        \n","        bs = embedded_1.size()[1]\n","        outputs_1, hidden_1 = self.lstm_1(embedded_1, hidden)\n","        outputs_2, hidden_2 = self.lstm_1(embedded_2, hidden)\n","        \n","        max_pool_1 = F.adaptive_avg_pool1d(outputs_1.permute(1,2,0),1).view(batch_size,-1)\n","        max_pool_2 = F.adaptive_avg_pool1d(outputs_2.permute(1,2,0),1).view(batch_size,-1)\n","        \n","        att_weights = torch.bmm(max_pool_1.view(batch_size, 1, 100), outputs_2.view(batch_size, 100, input_len)).view(batch_size, input_len)\n","        \n","        att_softmax = torch.zeros([batch_size, input_len])\n","        for i in range(batch_size):\n","          att_softmax[i] = F.softmax(att_weights[i], dim = 0)\n","        \n","        new_pool = torch.bmm(att_softmax.view(batch_size, 1, input_len), outputs_2.view(batch_size, input_len, 100).cpu()).view(batch_size, 100).cuda()\n","        \n","        ehs_1 = []\n","        for i in range(batch_size):\n","            e_list = []\n","            for j in range(len(commonList[i][0])):\n","                x = commonList[i][0][j]\n","              \n","                e_list.append(outputs_1[x][i])\n","            if len(e_list) > 0:\n","                mp1 = max_pool(e_list)\n","            else:\n","                mp1 = torch.zeros(100)\n","              \n","            ehs_1.append(mp1.cuda())\n","        \n","        \n","        ehs_2 = []\n","        for i in range(batch_size):\n","            e_list = []\n","            for j in range(len(commonList[i][1])):\n","                x = commonList[i][1][j]\n","              \n","                e_list.append(outputs_2[x][i])\n","            if len(e_list) > 0:\n","                mp2 = max_pool(e_list)\n","            else:\n","                mp2 = torch.zeros(100)\n","              \n","            ehs_2.append(mp2.cuda())\n","        \n","        '''ths_1 = torch.zeros(batch_size, 200)\n","        for i in range(batch_size):\n","            ths_1[i] = torch.cat((max_pool_1[i], ehs_1[i]),0)\n","          \n","        ths_2 = torch.zeros(batch_size, 200)\n","        for i in range(batch_size):\n","            ths_2[i] = torch.cat((max_pool_2[i], ehs_2[i]),0)'''\n","            \n","            \n","        elitehs_1 = torch.zeros(batch_size, 100)\n","        for i in range(batch_size):\n","            elitehs_1[i] = ehs_1[i]\n","          \n","        elitehs_2 = torch.zeros(batch_size, 100)\n","        for i in range(batch_size):\n","            elitehs_2[i] = ehs_2[i]\n","          \n","        #ths_1.cuda()\n","        #ths_2.cuda()\n","        elitehs_1.cuda()\n","        elitehs_2.cuda()\n","        #similarity_scores = self.exponent_neg_manhattan_distance(ths_1.cuda(), ths_2.cuda())\n","        similarity_scores = self.exponent_neg_manhattan_distance(max_pool_1, new_pool)\n","        \n","        return similarity_scores, elitehs_1, elitehs_2\n","    \n","    def init_weights(self):\n","        for name_1, param_1 in self.lstm_1.named_parameters():\n","            if 'bias' in name_1:\n","                nn.init.constant_(param_1, 0.0)\n","            elif 'weight' in name_1:\n","                nn.init.xavier_normal_(param_1)\n","\n","        lstm_1 = self.lstm_1.state_dict()\n","        lstm_2 = self.lstm_2.state_dict()\n","\n","        for name_1, param_1 in lstm_1.items():\n","            # Backwards compatibility for serialized parameters.\n","            if isinstance(param_1, torch.nn.Parameter):\n","                param_1 = param_1.data\n","\n","            lstm_2[name_1].copy_(param_1)\n","\n","    def init_hidden(self, batch_size):\n","        # Hidden dimensionality : 2 (h_0, c_0) x Num. Layers * Num. Directions x Batch Size x Hidden Size\n","        result = torch.zeros(2, 2, batch_size, self.hidden_size)\n","        result = tuple(result)\n","\n","        if self.use_cuda: \n","            result = (result[0].cuda(), result[1].cuda())\n","            return result\n","        else: return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g28vMmaYjcFW","colab":{}},"source":["model = Manhattan_LSTM(hidden_size, embedding.embedding_matrix, train_embedding=False)\n","if use_cuda: model = model.cuda()\n","model.init_weights()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RMl3LLyLjcFZ","colab":{}},"source":["import time\n","import random\n","from torch import optim\n","import torch.nn.utils.rnn as rnn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AuMB-N9tjcFc","colab":{}},"source":["x_train = data.x_train\n","x_val = data.x_val\n","y_train = data.y_train\n","y_val = data.y_val\n","x_test = data.x_test\n","y_test = data.y_test\n","train_samples = len(x_train)\n","val_samples = len(x_val)\n","test_samples = len(x_test)\n","test_samples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kC8RuPGejcFj","colab":{}},"source":["criterion = nn.BCELoss()\n","print_every = 1\n","print_loss_total = 0.0\n","train_loss = 0.0\n","val_loss = 0.0\n","max_acc = 0.73"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pNYiWg2rjcFl","colab":{}},"source":["model_trainable_parameters = tuple(filter(lambda p: p.requires_grad, model.parameters()))\n","model_optimizer = optim.Adam(model_trainable_parameters, lr=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Jp9aPy6JjcFn","colab":{}},"source":["hidden = model.init_hidden(batch_size)\n","len(hidden[0][0][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"89muNWJcjcFp","colab":{}},"source":["from helper import Helper\n","help_fn = Helper()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFGVjS5lJJwM","colab_type":"code","outputId":"212d93ba-61d3-4d3c-ea61-6e5d768dec1b","executionInfo":{"status":"ok","timestamp":1559376229763,"user_tz":-330,"elapsed":9877,"user":{"displayName":"HERSH DHILLON","photoUrl":"","userId":"06580087573372678514"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["#run to load the base model\n","model.load_state_dict(torch.load(\"./model_weights_base.pt\"))\n","model.eval()\n","model.train()\n","netG.load_state_dict(torch.load(\"./netG_weights_base.pt\"))\n","netG.eval()\n","netG.train()\n","netD.load_state_dict(torch.load(\"./netD_weights_base.pt\"))\n","netD.eval()\n","netD.train()\n","net_final.load_state_dict(torch.load(\"./netfinal_weights_base.pt\"))\n","net_final.eval()\n","net_final.train()"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Final_layer(\n","  (main): Sequential(\n","    (0): Linear(in_features=101, out_features=2, bias=True)\n","    (1): Softmax()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZU2WrGvXjcFr","colab":{}},"source":["start = time.time()\n","print('Beginning Model Training.\\n')\n","batch_size = 16\n","\n","for epoch in range(0, num_iters):\n","    model_loss = 0.0\n","    gen_loss = 0.0\n","    dis_loss = 0.0\n","    fin_loss = 0.0\n","    for i in range(0, train_samples, batch_size):\n","        input_variables = x_train[i:i+batch_size]\n","        similarity_scores = y_train[i:i+batch_size]\n","        \n","        sequences_1 = [sequence[0] for sequence in input_variables]\n","        sequences_2 = [sequence[1] for sequence in input_variables]\n","        batch_size = len(sequences_1)\n","        \n","        # Make a tensor for the similarity scores\n","        \n","        sim_scores_2d = torch.zeros([batch_size, 2])\n","        for j in range(batch_size):\n","          if similarity_scores[j] == 0:\n","            sim_scores_2d[j] = fake_label\n","          else:\n","            sim_scores_2d[j] = real_label\n","            \n","        sim_scores_2d = sim_scores_2d.cuda()\n","\n","        temp = rnn.pad_sequence(sequences_1 + sequences_2)\n","        sequences_1 = temp[:, :batch_size]\n","        sequences_2 = temp[:, batch_size:]\n","\n","        model_optimizer.zero_grad()\n","        loss_s = 0.0\n","        \n","        optimizerG.zero_grad()\n","        loss_g= 0.0\n","        \n","        optimizerD.zero_grad()\n","        loss_d = 0.0\n","        \n","        optimizer_final.zero_grad()\n","        loss_f = 0.0\n","\n","        # Initialise the hidden state and pass through the maLSTM\n","        hidden = model.init_hidden(batch_size)\n","        output_scores, ehs1, ehs2 = model([sequences_1, sequences_2], hidden)\n","        \n","        output_scores = output_scores.view(-1)\n","        \n","        loss_s += criterion(output_scores, similarity_scores)\n","        \n","        ehs1 = ehs1.cuda()\n","        ehs2 = ehs2.cuda()\n","        \n","        \n","        # Generator\n","        gen_feature = netG(ehs2)\n","        \n","        # 1. Discriminator for the real class\n","        discrimm_classes = netD(ehs1)\n","        labels = torch.zeros(batch_size, 2)\n","        for j in range(batch_size):\n","          labels[j] = real_label\n","          \n","        labels = labels.cuda()\n","        \n","        loss_d += criterion(discrimm_classes, labels)\n","        \n","        \n","        # 2. Discriminator for the fake class\n","        discrimm_classes = netD(gen_feature)\n","        labels = torch.zeros(batch_size, 2)\n","        for j in range(batch_size):\n","          labels[j] = fake_label\n","          \n","        labels = labels.cuda()\n","          \n","        loss_d += criterion(discrimm_classes, labels)\n","        \n","        #print(discrimm_classes)\n","        \n","        # Update generator loss\n","        loss_g += criterion(discrimm_classes, sim_scores_2d)\n","        \n","        d_feature = dropout_layer(gen_feature)\n","        \n","        cat_feature = torch.zeros([batch_size, len(d_feature[0])+1])\n","        for j in range(batch_size):\n","          for k in range(100):\n","            cat_feature[j][k] = d_feature[j][k]\n","          cat_feature[j][100] = output_scores[j]\n","          \n","        \n","        cat_feature = cat_feature.cuda()\n","        \n","        final_labels = net_final(cat_feature)\n","        \n","        loss_f += criterion(final_labels, sim_scores_2d)\n","        \n","        com_loss = loss_d + (0.5*loss_g) + loss_f + loss_s\n","        \n","        com_loss.backward()\n","        \n","        model_optimizer.step()\n","        optimizerG.step()\n","        optimizerD.step()\n","        optimizer_final.step()\n","        \n","        \n","        fin_loss += loss_f\n","        model_loss += loss_s\n","        gen_loss += loss_g\n","        dis_loss += loss_d\n","    \n","        train_loss = com_loss\n","        print_loss_total += com_loss\n","        \n","        \n","    '''if epoch % 5:\n","        learning_rate *= 0.5\n","        model_optimizer = optim.Adam(model_trainable_parameters, lr=learning_rate)\n","        optimizer_final = optim.Adam(net_final.parameters(), lr = learning_rate)\n","        optimizerD = optim.Adam(netD.parameters(), lr=learning_rate)\n","        optimizerG = optim.Adam(netG.parameters(), lr=learning_rate)\n","        '''\n","    \n","    a_scores = []\n","    p_scores = []\n","    corr = 0\n","    for i in range(0, test_samples, batch_size):\n","        input_variables = x_test[i:i+batch_size]\n","        actual_scores = y_test[i:i+batch_size]\n","\n","        sequences_1 = [sequence[0] for sequence in input_variables]\n","        sequences_2 = [sequence[1] for sequence in input_variables]\n","        batch_size = len(sequences_1)\n","        \n","        sim_scores_2d = torch.zeros([batch_size, 2])\n","        for j in range(batch_size):\n","          if similarity_scores[j] == 0:\n","            sim_scores_2d[j] = fake_label\n","          else:\n","            sim_scores_2d[j] = real_label\n","            \n","        sim_scores_2d = sim_scores_2d.cuda()\n","\n","        temp = rnn.pad_sequence(sequences_1 + sequences_2)\n","        sequences_1 = temp[:, :batch_size]\n","        sequences_2 = temp[:, batch_size:]\n","\n","        loss = 0.0\n","        hidden = model.init_hidden(batch_size)\n","        output_scores, ehs1, ehs2 = model([sequences_1, sequences_2], hidden)\n","        \n","        output_scores = output_scores.view(-1)\n","        \n","        ehs2 = ehs2.cuda() \n","        gen_feature = netG(ehs2)\n","        \n","        d_feature = dropout_layer(gen_feature)\n","        \n","        cat_feature = torch.zeros([batch_size, len(d_feature[0])+1])\n","        for j in range(batch_size):\n","          for k in range(100):\n","            cat_feature[j][k] = d_feature[j][k]\n","          cat_feature[j][100] = output_scores[j]\n","          \n","        cat_feature = cat_feature.cuda()\n","        \n","        final_labels = net_final(cat_feature)\n","        \n","        \n","        for j in range(0, batch_size):\n","          acts = actual_scores[j].data.cpu().numpy()\n","          preds = final_labels[j].data.cpu().numpy()\n","          a_scores.append(acts)\n","\n","          if preds[0] >= 0.5 and acts == 0:\n","            corr = corr+1\n","            p_scores.append(0)\n","          elif preds[1] >= 0.5 and acts == 1:\n","            corr = corr+1\n","            p_scores.append(1)\n","          elif preds[0] >=0.5:\n","            p_scores.append(0)\n","          else:\n","            p_scores.append(1)\n","          \n","    \n","    if epoch % print_every == 0:\n","        print_loss_avg = print_loss_total / print_every\n","        print_loss_total = 0\n","        print('%s (%d) %.4f' % (help_fn.time_slice(start, (epoch+1) / num_iters), epoch, print_loss_avg))\n","        print(\"LSTM loss    \" + str(model_loss.data.cpu().numpy()) + \"    Gen loss    \" + str(gen_loss.data.cpu().numpy()) + \"    Dis loss    \" + str(dis_loss.data.cpu().numpy()) + \"    Fin loss    \" + str(fin_loss.data.cpu().numpy()))\n","        print(\" Test Accuracy    \" + str(corr/len(a_scores)) + \"    f1 score    \" + str(f1_score(p_scores, a_scores)))\n","        \n","        acc = corr/len(a_scores)\n","        \n","        if acc > max_acc :\n","          max_acc = acc\n","          torch.save(model.state_dict(), \"./model_weights.pt\")\n","          torch.save(netG.state_dict(), \"./netG_weights.pt\" )\n","          torch.save(netD.state_dict(), \"./netD_weights.pt\")\n","          torch.save(net_final.state_dict(), \"./netfinal_weights.pt\")\n","          print(\"Model Saved!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMw_cFUi0y2i","colab_type":"code","outputId":"af1589bb-b7c3-400c-81e1-256a0ff50830","executionInfo":{"status":"ok","timestamp":1559239255374,"user_tz":-330,"elapsed":1209933,"user":{"displayName":"HERSH DHILLON","photoUrl":"","userId":"06580087573372678514"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["torch.save(model.state_dict(), \"./model_weights_base.pt\")\n","torch.save(netG.state_dict(), \"./netG_weights_base.pt\")\n","torch.save(netD.state_dict(), \"./netD_weights_base.pt\")\n","torch.save(net_final.state_dict(), \"./netfinal_weights_base.pt\")\n","print(\"Model Saved!\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model Saved!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8flebOA1DGBU","colab_type":"code","outputId":"9e0c1fd5-5040-4ce1-b2b0-823d8a093a50","executionInfo":{"status":"ok","timestamp":1559216295217,"user_tz":-330,"elapsed":9,"user":{"displayName":"HERSH DHILLON","photoUrl":"","userId":"06580087573372678514"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(max_acc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.73\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j6Yn390kS0gM","colab_type":"code","outputId":"1ac4ce06-ad91-45ff-db2e-81daa2165321","executionInfo":{"status":"ok","timestamp":1559130382896,"user_tz":-330,"elapsed":1053,"user":{"displayName":"HERSH DHILLON","photoUrl":"","userId":"06580087573372678514"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["#run to load the best model\n","model.load_state_dict(torch.load(\"./model_weights.pt\"))\n","model.eval()\n","model.train()\n","netG.load_state_dict(torch.load(\"./netG_weights.pt\"))\n","netG.eval()\n","netG.train()\n","netD.load_state_dict(torch.load(\"./netD_weights.pt\"))\n","netD.eval()\n","netD.train()\n","net_final.load_state_dict(torch.load(\"./netfinal_weights.pt\"))\n","net_final.eval()\n","net_final.train()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Final_layer(\n","  (main): Sequential(\n","    (0): Linear(in_features=101, out_features=2, bias=True)\n","    (1): Softmax()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"KmTi4UGzZxO9","colab_type":"code","colab":{}},"source":["learning_rate = 0.0003\n","#model_optimizer = optim.Adam(model_trainable_parameters, lr=learning_rate)\n","optimizer_final = optim.Adam(net_final.parameters(), lr = learning_rate)\n","optimizerD = optim.Adam(netD.parameters(), lr=learning_rate)\n","optimizerG = optim.Adam(netG.parameters(), lr=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YwWX8xbgZXZX","colab_type":"code","outputId":"915030c9-f3ea-454d-a10d-a693cc6366b2","colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["\"Only generator-discriminator training\"\n","start = time.time()\n","print('Beginning Model Training.\\n')\n","batch_size = 16\n","\n","for epoch in range(0, num_iters):\n","    model_loss = 0.0\n","    gen_loss = 0.0\n","    dis_loss = 0.0\n","    fin_loss = 0.0\n","    for i in range(0, train_samples, batch_size):\n","        input_variables = x_train[i:i+batch_size]\n","        similarity_scores = y_train[i:i+batch_size]\n","        \n","        sequences_1 = [sequence[0] for sequence in input_variables]\n","        sequences_2 = [sequence[1] for sequence in input_variables]\n","        batch_size = len(sequences_1)\n","        \n","        # Make a tensor for the similarity scores\n","        \n","        sim_scores_2d = torch.zeros([batch_size, 2])\n","        for j in range(batch_size):\n","          if similarity_scores[j] == 0:\n","            sim_scores_2d[j] = fake_label\n","          else:\n","            sim_scores_2d[j] = real_label\n","            \n","        sim_scores_2d = sim_scores_2d.cuda()\n","\n","        temp = rnn.pad_sequence(sequences_1 + sequences_2)\n","        sequences_1 = temp[:, :batch_size]\n","        sequences_2 = temp[:, batch_size:]\n","\n","        if model_optimizer: model_optimizer.zero_grad()\n","        loss_s = 0.0\n","        \n","        if optimizerG: optimizerG.zero_grad()\n","        loss_g= 0.0\n","        \n","        if optimizerD: optimizerD.zero_grad()\n","        loss_d = 0.0\n","        \n","        if optimizer_final: optimizer_final.zero_grad()\n","        loss_f = 0.0\n","\n","        # Initialise the hidden state and pass through the maLSTM\n","        hidden = model.init_hidden(batch_size)\n","        output_scores, ehs1, ehs2 = model([sequences_1, sequences_2], hidden)\n","        \n","        output_scores = output_scores.view(-1)\n","        \n","        loss_s += criterion(output_scores, similarity_scores)\n","        \n","        ehs1 = ehs1.cuda()\n","        ehs2 = ehs2.cuda()\n","        \n","        \n","        # Generator\n","        gen_feature = netG(ehs2)\n","        \n","        # 1. Discriminator for the real class\n","        discrimm_classes = netD(ehs1)\n","        labels = torch.zeros(batch_size, 2)\n","        for j in range(batch_size):\n","          labels[j] = real_label\n","          \n","        labels = labels.cuda()\n","        \n","        loss_d += criterion(discrimm_classes, labels)\n","        \n","        \n","        # 2. Discriminator for the fake class\n","        discrimm_classes = netD(gen_feature)\n","        labels = torch.zeros(batch_size, 2)\n","        for j in range(batch_size):\n","          labels[j] = fake_label\n","          \n","        labels = labels.cuda()\n","          \n","        loss_d += criterion(discrimm_classes, labels)\n","        \n","        #print(discrimm_classes)\n","        \n","        # Update generator loss\n","        loss_g += criterion(discrimm_classes, sim_scores_2d)\n","        \n","        d_feature = dropout_layer(gen_feature)\n","        \n","        cat_feature = torch.zeros([batch_size, len(d_feature[0])+1])\n","        for j in range(batch_size):\n","          for k in range(100):\n","            cat_feature[j][k] = d_feature[j][k]\n","          cat_feature[j][100] = output_scores[j]\n","          \n","        \n","        cat_feature = cat_feature.cuda()\n","        \n","        final_labels = net_final(cat_feature)\n","        \n","        loss_f += criterion(final_labels, sim_scores_2d)\n","        \n","        com_loss = loss_d + (0.5*loss_g) + loss_f\n","        \n","        com_loss.backward()\n","        #model_optimizer.step()\n","        optimizerG.step()\n","        optimizerD.step()\n","        optimizer_final.step()\n","        \n","        \n","        fin_loss += loss_f\n","        model_loss += loss_s\n","        gen_loss += loss_g\n","        dis_loss += loss_d\n","    \n","        train_loss = com_loss\n","        print_loss_total += com_loss\n","        \n","        \n","    '''if epoch % 5:\n","        learning_rate *= 0.5\n","        model_optimizer = optim.Adam(model_trainable_parameters, lr=learning_rate)\n","        optimizer_final = optim.Adam(net_final.parameters(), lr = learning_rate)\n","        optimizerD = optim.Adam(netD.parameters(), lr=learning_rate)\n","        optimizerG = optim.Adam(netG.parameters(), lr=learning_rate)\n","        '''\n","    \n","    a_scores = []\n","    p_scores = []\n","    corr = 0\n","    for i in range(0, test_samples, batch_size):\n","        input_variables = x_test[i:i+batch_size]\n","        actual_scores = y_test[i:i+batch_size]\n","\n","        sequences_1 = [sequence[0] for sequence in input_variables]\n","        sequences_2 = [sequence[1] for sequence in input_variables]\n","        batch_size = len(sequences_1)\n","        \n","        sim_scores_2d = torch.zeros([batch_size, 2])\n","        for j in range(batch_size):\n","          if similarity_scores[j] == 0:\n","            sim_scores_2d[j] = fake_label\n","          else:\n","            sim_scores_2d[j] = real_label\n","            \n","        sim_scores_2d = sim_scores_2d.cuda()\n","\n","        temp = rnn.pad_sequence(sequences_1 + sequences_2)\n","        sequences_1 = temp[:, :batch_size]\n","        sequences_2 = temp[:, batch_size:]\n","\n","        loss = 0.0\n","        hidden = model.init_hidden(batch_size)\n","        output_scores, ehs1, ehs2 = model([sequences_1, sequences_2], hidden)\n","        \n","        output_scores = output_scores.view(-1)\n","        \n","        ehs2 = ehs2.cuda() \n","        gen_feature = netG(ehs2)\n","        \n","        d_feature = dropout_layer(gen_feature)\n","        \n","        cat_feature = torch.zeros([batch_size, len(d_feature[0])+1])\n","        for j in range(batch_size):\n","          for k in range(100):\n","            cat_feature[j][k] = d_feature[j][k]\n","          cat_feature[j][100] = output_scores[j]\n","          \n","        cat_feature = cat_feature.cuda()\n","        \n","        final_labels = net_final(cat_feature)\n","        \n","        \n","        for j in range(0, batch_size):\n","          acts = actual_scores[j].data.cpu().numpy()\n","          preds = final_labels[j].data.cpu().numpy()\n","          a_scores.append(acts)\n","\n","          if preds[0] >= 0.5 and acts == 0:\n","            corr = corr+1\n","            p_scores.append(0)\n","          elif preds[1] >= 0.5 and acts == 1:\n","            corr = corr+1\n","            p_scores.append(1)\n","          elif preds[0] >=0.5:\n","            p_scores.append(0)\n","          else:\n","            p_scores.append(1)\n","          \n","    \n","    if epoch % print_every == 0:\n","        print_loss_avg = print_loss_total / print_every\n","        print_loss_total = 0\n","        print('%s (%d) %.4f' % (help_fn.time_slice(start, (epoch+1) / num_iters), epoch, print_loss_avg))\n","        print(\"LSTM loss    \" + str(model_loss.data.cpu().numpy()) + \"    Gen loss    \" + str(gen_loss.data.cpu().numpy()) + \"    Dis loss    \" + str(dis_loss.data.cpu().numpy()) + \"    Fin loss    \" + str(fin_loss.data.cpu().numpy()))\n","        print(\" Test Accuracy    \" + str(corr/len(a_scores)) + \"    f1 score    \" + str(f1_score(p_scores, a_scores)))\n","        \n","        acc = corr/len(a_scores)\n","        \n","        if acc > max_acc :\n","          max_acc = acc\n","          torch.save(model.state_dict(), \"./model_weights.pt\")\n","          torch.save(netG.state_dict(), \"./netG_weights.pt\")\n","          torch.save(netD.state_dict(), \"./netD_weights.pt\")\n","          torch.save(net_final.state_dict(), \"./netfinal_weights.pt\")\n","          print(\"Model Saved!\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Beginning Model Training.\n","\n","3m 16s (- 29m 25s) (0) 1423.5413\n","LSTM loss    109.96675    Gen loss    272.357    Dis loss    67.64587    Fin loss    108.48346\n"," Test Accuracy    0.7275362318840579    f1 score    0.8183925811437404\n","6m 19s (- 25m 16s) (1) 550.9258\n","LSTM loss    193.8971    Gen loss    483.91293    Dis loss    120.182976    Fin loss    188.78645\n"," Test Accuracy    0.7194202898550724    f1 score    0.8062449959967974\n","9m 5s (- 21m 11s) (2) 1633.8693\n","LSTM loss    572.37994    Gen loss    1454.5221    Dis loss    360.27142    Fin loss    546.337\n"," Test Accuracy    0.7089855072463768    f1 score    0.7744833782569632\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s9rr25MbZ9x7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}